{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras import models, Model, layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''define parameters here'''\n",
    "chain_lenght = 21               #spin chain lenght\n",
    "nodes = [500,200,100]           #network nodes\n",
    "epochs = 600                    #epochs\n",
    "batch_size = 100                #batch\n",
    "outputlayer_shape = 4           #number of parameters to predict\n",
    "num_of_dcs = 3                  #number of different dcs\n",
    "num_of_Es = 266                 #number of energies in dI/dV curve\n",
    "es_dI2d2V = np.linspace(-0.5,4,300)\n",
    "es_dIdV = np.linspace(0,4,266)\n",
    "include_gamma = False           #include gamma to the training\n",
    "d_ind = (1, 0.05)               #dropout layer index and %, no dropout = (0,0)\n",
    "loss = 'mean_squared_error'     #loss function                                     \n",
    "n_params = 6                    #number of parameters\n",
    "SEED = 3                        #random seed for tensorflow\n",
    "noise_seed = 1                  #random seed for noisy data\n",
    "include_all = False             #training with all datasets/only single one, True/False\n",
    "PCA_noise = 3                   #amount of gaussian noise in PCA fitting\n",
    "RELU = True                     #relu activation function\n",
    "noise_scaler = 0.01             #noise scaler for noisy copies\n",
    "noise_levels = [level for level in range(11)] # #noise levels for noisy copies in training set\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()        #deterministic in order to repeat results\n",
    "tf.random.set_seed(SEED)                              #SEED for tensorflow   \n",
    "\n",
    "network = f'{noise_seed}_{SEED}'                      #network name with params\n",
    "name = f'SingleImp'                                   #name of the network for saving files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heat(chain_lenght: int, dcs: np.array, es: np.linspace, num_of_dcs: int, num_of_Es : int, sample:  int, title: str, norm_max: float, method: str, directions: list):\n",
    "    '''Function to plot heatmaps of dynamical correlators.\n",
    "    Args:\n",
    "        chain_lenght (int): Length of the spin chain.\n",
    "        dcs (np.array): Array of dynamical correlators.\n",
    "        es (np.linspace): Energy values.\n",
    "        num_of_dcs (int): Number of dynamical correlators in the dataset (1, 2, or 3).\n",
    "        num_of_Es (int): Number of energy values.\n",
    "        sample (int): Sample index.\n",
    "        title (str): Title for the plot.\n",
    "        norm_max (float): Maximum normalization value for the plot.\n",
    "        method (str): Interpolation method for the griddata function.\n",
    "        directions (list): List of directions for the dynamical correlators.\n",
    "    Returns:\n",
    "        None: The function saves the plot as a PNG file and displays it.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Takes the chain length, dynamical correlators, energy values, number of dynamical correlators, number of energies, sample index, \n",
    "       title for the plot, maximum normalization value, interpolation method, and directions.\n",
    "    2. Loops over each dynamical correlator direction.\n",
    "    3. For each direction, it creates a file \"XYZ.OUT\" to store the data.\n",
    "    4. Loops over each site in the chain and for each energy value, it writes the site number, energy value, \n",
    "       and corresponding dynamical correlator value to the file.'''\n",
    "\n",
    "    '''loop over dynamical correlators'''\n",
    "    custom_cmap = cm.get_cmap('inferno')  \n",
    "\n",
    "\n",
    "    for dc in range(num_of_dcs):\n",
    "        f = open(\"XYZ.OUT\",\"w\")\n",
    "\n",
    "        '''loop over sites'''\n",
    "        for s in range(chain_lenght): \n",
    "            for idx, freq in enumerate(es):\n",
    "                D = dcs[sample,(num_of_dcs*s+dc)*num_of_Es:(num_of_dcs*s+dc)*num_of_Es+num_of_Es]\n",
    "                \n",
    "                f.write(str(s)+\"  \")\n",
    "                f.write(str(freq)+\"  \")\n",
    "                f.write(str(D[idx])+\"\\n\")\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        '''create mesh'''\n",
    "        x = np.linspace(0,chain_lenght-1,num_of_Es)\n",
    "        y =  es\n",
    "        x_mesh, y_mesh = np.meshgrid(x,y)\n",
    "\n",
    "        d = np.genfromtxt(\"XYZ.OUT\").T\n",
    "        norm = plt.Normalize(0,norm_max)\n",
    "\n",
    "        mesh_plot = plt.figure()\n",
    "        z_mesh = griddata((d[0], d[1]), d[2], (x_mesh, y_mesh), method=method)\n",
    "        plt.contourf(x_mesh, y_mesh, z_mesh, 100, norm=norm, cmap=custom_cmap)\n",
    "        plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "        \n",
    "        c = plt.colorbar()\n",
    "        c.ax.tick_params(labelsize=16)\n",
    "\n",
    "\n",
    "        plt.title(f\"{title} {directions[dc]}\", fontsize=18)\n",
    "        plt.xlabel('Site number', fontsize=16)\n",
    "        plt.ylabel('Energy [E]', fontsize=16)\n",
    "        plt.savefig(f'spinchain{chain_lenght}_mesh_plot_{dc}_{sample}_{directions[dc]}_.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "def extend_with_reversed_subsets(arr, num_of_energies):\n",
    "    ''''Function to extend dynamical correlators for the whole symmetric chain.\n",
    "    Arguments:\n",
    "        arr (np.array): Input array with shape (n_samples, n_features). Input array has dynamical correlators for one half of the chain in one spatial direction.\n",
    "        num_of_energies (int): Size of the subsets to create. Grid point number in the dynamical correlators.\n",
    "    Returns:\n",
    "        np.array: Extended array with shape (n_samples, new_features). Output array has dynamical correlators for the whole chain.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Takes an array of dynamical correlators for one half of the chain.\n",
    "    2. Creates subsets of the array based on the number of energies.\n",
    "    3. Reverses the subsets and concatenates them to create an extended array for the whole chain.\n",
    "    4. Returns the extended array with dynamical correlators for the whole chain.\n",
    "    5. The output array has dynamical correlators for the whole chain, first x and then z direction for each spin.\n",
    "\n",
    "    Args:\n",
    "        arr (np.array): Input array with shape (n_samples, n_features). Input array has dynamical correlators for one half of the chain in one spatial direction.\n",
    "        num_of_energies (int): Size of the subsets to create. Grid point number in the dynamical correlators.\n",
    "    Returns:\n",
    "        np.array: Extended array with shape (n_samples, new_features). Output array has dynamical correlators for the whole chain'''\n",
    "    \n",
    "    num_cols = arr.shape[1]                \n",
    "    num_spins = num_cols // num_of_energies   \n",
    "    subsets = [arr[:, i*num_of_energies:(i+1)*num_of_energies] for i in range(num_spins)]     #create subset \n",
    "    reversed_subsets = subsets[:-1][::-1]                                                     #exclude last subset and reverse\n",
    "    extended_dIdV = np.hstack(subsets + reversed_subsets)                                     #concatenate data\n",
    "\n",
    "    return extended_dIdV\n",
    "\n",
    "def include_dc(dcs_all, dc_dir:int, chain_length, n_d, n_E):\n",
    "    '''Function to include a specific dynamical correlator direction in the dataset.\n",
    "    Args:\n",
    "        dcs_all (np.array): Array of dynamical correlators with shape (n_samples, n_d * n_E * chain_length).\n",
    "        dc_dir (int): Index of the dynamical correlator direction to include.\n",
    "        chain_length (int): Length of the spin chain.\n",
    "        n_d (int): Number of dynamical correlators.\n",
    "        n_E (int): Number of energy values.\n",
    "    Returns:\n",
    "        np.array: Array with the specified dynamical correlator direction for all chains.\n",
    "        \n",
    "    This function performs the following steps:\n",
    "    1. Takes an array of dynamical correlators, direction index, chain length, number of dynamical correlators, and number of energies.\n",
    "    2. Initializes an empty array to hold the selected dynamical correlator direction for all chains.\n",
    "    3. Iterates through each chain and extracts the specified dynamical correlator direction.\n",
    "    4. Concatenates the extracted dynamical correlator direction for all chains into a single array.\n",
    "    5. Returns the concatenated array with the specified dynamical correlator direction for all chains.'''\n",
    "    \n",
    "    dc_dir_ALL = np.zeros((dcs_all.shape[0], n_E * chain_length))\n",
    "\n",
    "    for s in range(chain_length):\n",
    "        start_idx = s * n_d * n_E + dc_dir * n_E\n",
    "        end_idx = start_idx + n_E\n",
    "        dc = dcs_all[:, start_idx:end_idx]\n",
    "        dc_dir_ALL[:, s * n_E:(s + 1) * n_E] = dc\n",
    "\n",
    "    return dc_dir_ALL\n",
    "\n",
    "def compute_cumulative_integral(arr, sites, n_dcs, n_E_final, final_linspace, group_size=300):\n",
    "    '''Function to compute the cumulative integral of dI2/d2V for each sample in the dataset.\n",
    "    Args:\n",
    "        arr (np.array): Array of dI2/d2V values with shape (n_samples, n_features).\n",
    "        sites (int): Number of sites in the spin chain.\n",
    "        n_dcs (int): Number of dynamical correlators.\n",
    "        n_E_final (int): Number of final energy values.\n",
    "        final_linspace (np.linspace): Final linspace for the energy values.\n",
    "    group_size (int): Size of the groups to split the array into. Default is 300.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array with cumulative integrals for each sample with shape (n_samples, n_E_final * sites * n_dcs).\n",
    "        \n",
    "    This function performs the following steps:\n",
    "    1. Takes an array of dI2/d2V values, number of sites, number of dynamical correlators, number of final energies, and the final linspace.\n",
    "    2. Splits the array into chunks based on the group size.\n",
    "    3. Computes the cumulative integral for each chunk using the trapezoidal rule.\n",
    "    4. Concatenates the results to create a final array with the cumulative integral for each sample.\n",
    "    5. Returns the final array with cumulative integrals for each sample.\n",
    "    '''\n",
    "    dc_dir_ALL = np.zeros((arr.shape[0],n_E_final*sites*n_dcs))\n",
    "    \n",
    "    for sample in range(arr.shape[0]):\n",
    "        chunks = [arr[sample][i:i + group_size] for i in range(0, arr.shape[1], group_size)]\n",
    "        result = [chunk[(group_size-n_E_final):] for chunk in chunks if len(chunk) >= group_size]\n",
    "        \n",
    "        def compute_sum(dI2_d2V, es):\n",
    "            return cumulative_trapezoid(dI2_d2V, es, initial=0)\n",
    "        \n",
    "        dI_dVs = compute_sum(result[:], final_linspace)\n",
    "        dI_dV  = np.array(np.concatenate(dI_dVs))\n",
    "        dc_dir_ALL[sample] = dI_dV\n",
    "        \n",
    "    return dc_dir_ALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''download data and split train and test'''\n",
    "if include_all:\n",
    "    data10 = np.genfromtxt(f'dIdV_impurity_at_10.txt')\n",
    "    data911 = np.genfromtxt(f'dIdV_impurity_at_9and11.txt')\n",
    "    data812 = np.genfromtxt(f'dIdV_impurity_at_8and12.txt')\n",
    "\n",
    "    params = data812[:, len(data812[0])-n_params:]      #parameters are the same for all datasets, take from one\n",
    "\n",
    "    dcs_x_10  = data10[:, :2926]                         #dynamical correlators for x direction\n",
    "    dcs_z_10  = data10[:, 2926:-6]                       #dynamical correlators for z direction\n",
    "    dcs_x_911 = data911[:, :2926]\n",
    "    dcs_z_911 = data911[:, 2926:-6]\n",
    "    dcs_x_812 = data812[:, :2926]\n",
    "    dcs_z_812 = data812[:, 2926:-6]\n",
    "\n",
    "    dcs_x_10 = extend_with_reversed_subsets(dcs_x_10, 266)      #extend to whole chain\n",
    "    dcs_z_10 = extend_with_reversed_subsets(dcs_z_10, 266)\n",
    "    dcs_x_911 = extend_with_reversed_subsets(dcs_x_911, 266)\n",
    "    dcs_z_911 = extend_with_reversed_subsets(dcs_z_911, 266)\n",
    "    dcs_x_812 = extend_with_reversed_subsets(dcs_x_812, 266)\n",
    "    dcs_z_812 = extend_with_reversed_subsets(dcs_z_812, 266)\n",
    "\n",
    "    dcs_x = np.concatenate((dcs_x_10, dcs_x_911, dcs_x_812), axis=1)\n",
    "    dcs_z = np.concatenate((dcs_z_10, dcs_z_911, dcs_z_812), axis=1)\n",
    "\n",
    "    dIdV_and_params = np.concatenate((dcs_x, dcs_z, params), axis=1)\n",
    "\n",
    "    print(dIdV_and_params.shape)\n",
    "    train_data                  = dIdV_and_params[:dIdV_and_params.shape[0]-dIdV_and_params.shape[0]//3, :]\n",
    "    test_data                   = dIdV_and_params[dIdV_and_params.shape[0]-dIdV_and_params.shape[0]//3:, :]\n",
    "\n",
    "\n",
    "\n",
    "else: \n",
    "    dIdV_and_params = np.genfromtxt(f'dIdV_impurity_at_10.txt')\n",
    "\n",
    "    params = dIdV_and_params[:, len(dIdV_and_params[0])-n_params:]\n",
    "    dcs_x_10 = dIdV_and_params[:, :2926]\n",
    "    dcs_z_10 = dIdV_and_params[:, 2926:-6]\n",
    "\n",
    "    dcs_x_10 = extend_with_reversed_subsets(dcs_x_10, 266)\n",
    "    dcs_z_10 = extend_with_reversed_subsets(dcs_z_10, 266)\n",
    "\n",
    "    dIdV_and_params = np.concatenate((dcs_x_10, dcs_z_10, params), axis=1)\n",
    "\n",
    "    train_data                  = dIdV_and_params[:dIdV_and_params.shape[0]-dIdV_and_params.shape[0]//3, :]\n",
    "    test_data                   = dIdV_and_params[dIdV_and_params.shape[0]-dIdV_and_params.shape[0]//3:, :]\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise(x, mu, std, seed):\n",
    "    '''Function to add Gaussian noise to the input data.\n",
    "    Args:\n",
    "        x (np.array): Input data array with shape (n_samples, n_features).\n",
    "        mu (float): Mean of the Gaussian noise.\n",
    "        std (float): Standard deviation of the Gaussian noise.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        np.array: Noisy data array with the same shape as the input data.'''\n",
    "\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "    noise = np.random.normal(mu, std, size = x.shape)\n",
    "    x_noisy = x + noise\n",
    "    return x_noisy\n",
    "\n",
    "def noisy_copies(raw_data, n_copies, n_variations, n_params, n_levels, seed):\n",
    "    '''Function to create noisy copies of the raw data.\n",
    "    Args:\n",
    "        raw_data (np.array): Original data array with shape (n_samples, n_features).\n",
    "        n_copies (int): Number of copies to create.\n",
    "        n_variations (int): Number of variations for each copy.\n",
    "        n_params (int): Number of parameters in the dataset.\n",
    "        n_levels (list): List of noise levels to apply.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        np.array: Noisy data array with shape (n_copies * n_variations, n_features).\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Takes the raw data, number of copies, number of variations, number of parameters, noise levels, and seed.\n",
    "    2. Creates a noisy version of the raw data by tiling it according to the number of copies and variations.\n",
    "    3. Extracts the dynamical correlators and parameters from the noisy data.\n",
    "    4. Applies Gaussian noise to the dynamical correlators based on the specified noise levels.\n",
    "    5. Returns the modified dynamical correlators and parameters as separate arrays.\n",
    "    '''\n",
    "\n",
    "    noisy_data           = np.tile(raw_data, (n_copies*n_variations,1))\n",
    "    copied_dcs           = np.array(noisy_data[:,0:len(noisy_data[0])-n_params])\n",
    "    \n",
    "    copied_dcs[copied_dcs < 0] = 0\n",
    "    params               = noisy_data[:,len(noisy_data[0])-n_params:]\n",
    "    \n",
    "    for level in n_levels:\n",
    "        if level == 0:\n",
    "            continue\n",
    "        \n",
    "        s_ind = level*copied_dcs.shape[0]//n_copies\n",
    "        e_ind = (level+1)*copied_dcs.shape[0]//n_copies\n",
    "        \n",
    "        copied_dcs[s_ind:e_ind,:] = gaussian_noise(np.copy(copied_dcs[s_ind:e_ind,:]), mu=0.0, std=level * noise_scaler * np.std(copied_dcs), seed = seed)\n",
    "\n",
    "    return copied_dcs, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''generate noisy training set'''\n",
    "dcs_train, params_train     = noisy_copies(train_data, 11, 1, 6, noise_levels, seed=noise_seed)\n",
    "\n",
    "dcs_test                    = test_data[:,0:-n_params]\n",
    "params_test                 = test_data[:,-n_params:]\n",
    "\n",
    "dcs_train[dcs_train < 0]    = 0\n",
    "dcs_test[dcs_test < 0]      = 0\n",
    "\n",
    "dcs_train                   = np.array(dcs_train)\n",
    "dcs_test                    = np.array(dcs_test)\n",
    "\n",
    "J1_train      = params_train[:,0]\n",
    "J2_train      = params_train[:,1] \n",
    "JZ_train      = params_train[:,2]\n",
    "JD_train      = params_train[:,3]\n",
    "J3_train      = params_train[:,4]\n",
    "gamma_train   = params_train[:,5]\n",
    "\n",
    "J1_test      = params_test[:,0]\n",
    "J2_test      = params_test[:,1] \n",
    "JZ_test      = params_test[:,2]\n",
    "JD_test      = params_test[:,3]\n",
    "J3_test      = params_test[:,4]\n",
    "gamma_test   = params_test[:,5]\n",
    "\n",
    "\n",
    "'''fit PCA on noisy (chi=0.03) data and apply it to train and test sets'''\n",
    "pca = PCA(0.99)\n",
    "dcs_for_PCA = dcs_train[PCA_noise*280:(PCA_noise+1)*280,:]\n",
    "pca.fit(dcs_for_PCA)\n",
    "\n",
    "dcs_train_arr               = pca.transform(dcs_train)\n",
    "dcs_test_arr                = pca.transform(dcs_test)\n",
    "\n",
    "dcs_PCA_train   = np.copy(dcs_train_arr)\n",
    "dcs_PCA_test    = np.copy(dcs_test_arr)\n",
    "pca_components  = dcs_PCA_train.shape[1]\n",
    "\n",
    "with open(f'pca_{name}.pkl', 'wb') as pickle_file: \n",
    "    pickle.dump(pca, pickle_file)\n",
    "print(f\"number of PCA components used: {pca_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''define scalers'''\n",
    "scaler_J2   = MinMaxScaler(feature_range=(0,1)) \n",
    "scaler_JZ  = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_J3 = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_JD   = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "'''scale train params'''\n",
    "J2_train          = np.array(J2_train).reshape(-1,1)\n",
    "JZ_train          = np.array(JZ_train).reshape(-1,1)\n",
    "JD_train          = np.array(JD_train).reshape(-1,1)\n",
    "J3_train          = np.array(J3_train).reshape(-1,1)\n",
    "\n",
    "'''scale test params'''\n",
    "J2_test          = np.array(J2_test).reshape(-1,1)\n",
    "JZ_test          = np.array(JZ_test).reshape(-1,1)\n",
    "JD_test          = np.array(JD_test).reshape(-1,1)\n",
    "J3_test          = np.array(J3_test).reshape(-1,1)\n",
    "\n",
    "scaler_J2.fit(np.concatenate((J2_train, J2_test), axis = 0))\n",
    "scaler_JZ.fit(np.concatenate((JZ_train, JZ_test), axis = 0))\n",
    "scaler_J3.fit(np.concatenate((J3_train, J3_test), axis = 0))\n",
    "scaler_JD.fit(np.concatenate((JD_train, JD_test), axis = 0))\n",
    "\n",
    "J2_scaled_train   = scaler_J2.transform(J2_train)\n",
    "JZ_scaled_train   = scaler_JZ.transform(JZ_train)\n",
    "J3_scaled_train  = scaler_J3.transform(J3_train)\n",
    "JD_scaled_train   = scaler_JD.transform(JD_train)\n",
    "\n",
    "J2_scaled_test   = scaler_J2.transform(J2_test)\n",
    "JZ_scaled_test   = scaler_JZ.transform(JZ_test)\n",
    "J3_scaled_test  = scaler_J3.transform(J3_test)\n",
    "JD_scaled_test   = scaler_JD.transform(JD_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(list :list, input_shape, output_shape, d_ind, loss):\n",
    "    model = Sequential()\n",
    "    '''loop over nodes in the list'''\n",
    "    for idx, node in enumerate(list):\n",
    "\n",
    "        if idx == 0:\n",
    "            model.add(Dense(node, input_shape=input_shape, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED) , activation='relu'))\n",
    "            continue\n",
    "\n",
    "        '''adding dropout layer before dense layer (number idx)'''\n",
    "        if idx == d_ind[0] and idx != 0:\n",
    "            model.add(Dropout(rate=d_ind[1], noise_shape=None, seed=None))\n",
    "\n",
    "        model.add(Dense(node, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED), activation='relu'))\n",
    "\n",
    "    model.add(Dense(output_shape, kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED), activation= 'relu'))\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=[\"mae\", \"mse\"])\n",
    "    return model \n",
    "\n",
    "def plot_loss(history,name, log):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    if log == True:\n",
    "        plt.yscale('log')\n",
    "    plt.title('Loss errors as functions of epochs',fontsize=18)\n",
    "    plt.xlabel('Epoch',fontsize=18)\n",
    "    plt.ylabel('Error',fontsize=18)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{name}_loss', dpi = 300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def fit_and_evaluate(nodes, epochs, batch_size, X_test_scaled, y_test_scaled, X_train_scaled, y_train_scaled, input_shape, output_shape, d_ind, loss, name, log:bool):\n",
    "    NN_model = baseline_model(nodes, input_shape, output_shape, d_ind, loss)\n",
    "    history = NN_model.fit(X_train_scaled, y_train_scaled, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    results = NN_model.evaluate(X_test_scaled, y_test_scaled)\n",
    "    print(\"test loss, test mae:\", results)\n",
    "    NN_model.save(f'{name}.h5')\n",
    "\n",
    "    plot_loss(history,name,log)\n",
    "\n",
    "    J_predicted = NN_model.predict(X_test_scaled)\n",
    "    J_predicted_training = NN_model.predict(X_train_scaled)\n",
    "\n",
    "    return J_predicted, J_predicted_training\n",
    "\n",
    "def MSE(true,pred,param='none'):\n",
    "    MSE = mean_squared_error(y_true=true, y_pred=pred)\n",
    "    return float(format(MSE, \".4f\"))\n",
    "\n",
    "def MAE(true,pred,param='none'):\n",
    "    MAE = mean_absolute_error(y_true=true, y_pred=pred)\n",
    "    return float(format(MAE, \".4f\"))\n",
    "\n",
    "\n",
    "def compute_fidelity(true:np.array, pred:np.array):\n",
    "\n",
    "    E_pred_true    = np.mean(np.multiply(pred, true))\n",
    "    E_pred         = np.mean(pred)\n",
    "    E_true         = np.mean(true)\n",
    "    E_pred2        = np.mean(np.multiply(pred, pred))\n",
    "    E_true2        = np.mean(np.multiply(true, true))\n",
    "    \n",
    "    fidelity       = abs(E_pred_true - E_pred*E_true) / math.sqrt((E_true2 - E_true**2)*(E_pred2 - E_pred**2))\n",
    "    return float(format(fidelity, \".4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train size {dcs_PCA_train.shape}, test shape {dcs_PCA_test.shape}\")\n",
    "\n",
    "inputlayer_shape    = (pca_components,)\n",
    "X_train_scaled      = dcs_PCA_train\n",
    "X_test_scaled       = dcs_PCA_test\n",
    "\n",
    "y_train_scaled      = np.concatenate((J2_scaled_train, JZ_scaled_train, JD_scaled_train, J3_scaled_train), axis=1)\n",
    "y_test_scaled       = np.concatenate((J2_scaled_test, JZ_scaled_test, JD_scaled_test, J3_scaled_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''fit and evaluate the model'''\n",
    "J_predicted, J_predicted_training = fit_and_evaluate(nodes, epochs, batch_size, X_test_scaled, y_test_scaled, X_train_scaled, y_train_scaled, inputlayer_shape, outputlayer_shape, d_ind, loss, name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MSE and MAE of predictions'''\n",
    "print(MSE(J2_scaled_test, J_predicted[:,0:1], 'test J2'))\n",
    "print(MSE(J2_scaled_train, J_predicted_training[:,0:1], 'train J2'))\n",
    "print(MAE(J2_scaled_test, J_predicted[:,0:1], 'test J2'))\n",
    "print(MAE(J2_scaled_train, J_predicted_training[:,0:1], 'train J2'))\n",
    "\n",
    "print(MSE(JZ_scaled_test, J_predicted[:,1:2], 'test JZ'))\n",
    "print(MSE(JZ_scaled_train, J_predicted_training[:,1:2], 'train JZ'))\n",
    "print(MAE(JZ_scaled_test, J_predicted[:,1:2], 'test JZ'))\n",
    "print(MAE(JZ_scaled_train, J_predicted_training[:,1:2], 'train JZ'))\n",
    "\n",
    "print(MSE(JD_scaled_test, J_predicted[:,2:3], 'test JD'))\n",
    "print(MSE(JD_scaled_train, J_predicted_training[:,2:3], 'train JDMI'))\n",
    "print(MAE(JD_scaled_test, J_predicted[:,2:3], 'test JD'))\n",
    "print(MAE(JD_scaled_train, J_predicted_training[:,2:3], 'train JDMI'))\n",
    "\n",
    "print(MSE(J3_scaled_test, J_predicted[:,3:4], 'test J3'))\n",
    "print(MSE(J3_scaled_train, J_predicted_training[:,3:4], 'train J3'))\n",
    "print(MAE(J3_scaled_test, J_predicted[:,3:4], 'test J3'))\n",
    "print(MAE(J3_scaled_train, J_predicted_training[:,3:4], 'train J3'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''scale back to original range'''\n",
    "J2_predicted_inverse_scaled     = scaler_J2.inverse_transform(J_predicted[:,0:1])\n",
    "JZ_predicted_inverse_scaled     = scaler_JZ.inverse_transform(J_predicted[:,1:2])\n",
    "JD_predicted_inverse_scaled     = scaler_JD.inverse_transform(J_predicted[:,2:3])\n",
    "J3_predicted_inverse_scaled     = scaler_J3.inverse_transform(J_predicted[:,3:4])\n",
    "\n",
    "J2_predicted_training_scaled    = scaler_J2.inverse_transform(J_predicted_training[:,0:1])\n",
    "JZ_predicted_training_scaled    = scaler_JZ.inverse_transform(J_predicted_training[:,1:2])\n",
    "JD_predicted_training_scaled    = scaler_JD.inverse_transform(J_predicted_training[:,2:3])\n",
    "J3_predicted_training_scaled    = scaler_J3.inverse_transform(J_predicted_training[:,3:4])\n",
    "\n",
    "J2_test     = scaler_J2.inverse_transform(J2_scaled_test)\n",
    "JZ_test     = scaler_JZ.inverse_transform(JZ_scaled_test)\n",
    "JD_test     = scaler_JD.inverse_transform(JD_scaled_test)\n",
    "J3_test     = scaler_J3.inverse_transform(J3_scaled_test)\n",
    "J2_train    = scaler_J2.inverse_transform(J2_scaled_train)\n",
    "JZ_train    = scaler_JZ.inverse_transform(JZ_scaled_train)\n",
    "JD_train    = scaler_JD.inverse_transform(JD_scaled_train)\n",
    "J3_train    = scaler_J3.inverse_transform(J3_scaled_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baselines(baselines: list, name: str, chi: float, NN_name: str):\n",
    "    \"\"\"Function to plot baselines of predicted values compared to true values.\n",
    "    Args:\n",
    "        baselines (list): List of tuples containing parameter name, true values, and predicted values.\n",
    "        name (str): Name for saving the plot.\n",
    "        chi (float): Random noise value used in the dataset.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=len(baselines)//2, figsize = (10,5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, pred in enumerate(baselines):\n",
    "        axes[idx].scatter(pred[1], pred[2], c = 'blue')\n",
    "        axes[idx].set_title(rf'{pred[0]} [J$_1$]', fontsize=25)\n",
    "        axes[idx].plot(pred[1], pred[1], color='red')\n",
    "        fid = compute_fidelity(pred[2], pred[1])\n",
    "        axes[idx].text(0.2, 0.8, r'$\\mathcal{F}$'+ f' :{float(format(fid, \".2f\"))}', fontsize= 20, transform=axes[idx].transAxes, horizontalalignment='center', verticalalignment='center') \n",
    "        axes[idx].tick_params(axis='both', which='major', labelsize=23)\n",
    "    \n",
    "    fig.supxlabel('True values', fontsize=25, y=0.02)\n",
    "    fig.supylabel('Predicted values', fontsize=25, x=0.05)\n",
    "    fig.suptitle(rf'Predicted parameters with $\\chi$={chi} for {NN_name}', fontsize=25)\n",
    "    plt.subplots_adjust(left=0.17, bottom=0.18, right=0.98, top=0.82, wspace=0.35, hspace=0.66)\n",
    "    plt.savefig(f'baselines_'+name+'.png', dpi = 500)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baselines=[('J2', J2_test, J2_predicted_inverse_scaled), \n",
    "           ('JZ', JZ_test, JZ_predicted_inverse_scaled), \n",
    "           ('JDMI', JD_test, JD_predicted_inverse_scaled), \n",
    "           ('J3', J3_test, J3_predicted_inverse_scaled)]\n",
    "\n",
    "plot_baselines(baselines=baselines, name='scatter', chi=0, NN_name=name)\n",
    "\n",
    "predicted_data_save = np.concatenate((J2_predicted_inverse_scaled, JZ_predicted_inverse_scaled, JD_predicted_inverse_scaled, J3_predicted_inverse_scaled), axis=1)\n",
    "np.savetxt('predicted_results_'+name+'.txt', np.array(predicted_data_save))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'/Users/35840/Documents/CQM_2024/masters_project/{name}.h5')\n",
    "\n",
    "with open(f'pca_{name}.pkl', 'rb') as pickle_file: \n",
    "     pca_test = pickle.load(pickle_file)\n",
    "\n",
    "def compute_fid_with_seed(test, test_params, model_fid, chi_range, pca_i):\n",
    "    '''Function to compute fidelity of predictions with added noise and specified seed.\n",
    "    Args:\n",
    "        test (np.array): Test data array with shape (n_samples, n_features).\n",
    "        test_params (np.array): Test parameters array with shape (n_samples, n_params).\n",
    "        model_fid (tf.keras.Model): Trained model for predictions.\n",
    "        chi_range (np.array): Array of noise levels to apply.\n",
    "        pca_i (PCA): PCA object for transforming the test data.\n",
    "    Returns:\n",
    "        tuple: Tuple containing lists of fidelities and errors for each parameter, as well as noise values and parameter values.\n",
    "    '''\n",
    "    # Define ranges and initialize lists for results\n",
    "    fidelity_results = {'J2': [], 'JZ': [], 'JDM': [], 'J3': []}\n",
    "    error_results = {'J2': [], 'JZ': [], 'JDM': [], 'J3': []}\n",
    "\n",
    "    # Inverse transform test parameters\n",
    "    J2_test = scaler_J2.inverse_transform(test_params[:, 0:1].reshape(-1,1))\n",
    "    JZ_test = scaler_JZ.inverse_transform(test_params[:, 1:2].reshape(-1,1))\n",
    "    JD_test = scaler_JD.inverse_transform(test_params[:, 2:3].reshape(-1,1))\n",
    "    J3_test = scaler_J3.inverse_transform(test_params[:, 3:4].reshape(-1,1))\n",
    "\n",
    "    def compute_and_store_fidelity(fids, pred, true):\n",
    "        fids.append(compute_fidelity(true, pred))\n",
    "        \n",
    "    # Iterate over the noise range\n",
    "    for i, g in enumerate(chi_range):\n",
    "        dcs_gaussian = gaussian_noise(np.copy(test), mu=0.0, std= g * np.std(test), seed=i)\n",
    "\n",
    "        dcs_test_noisy_transformed = pca_i.transform(dcs_gaussian)\n",
    "        noisy_results = model_fid.predict(dcs_test_noisy_transformed)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        J2_pred_from_noise = scaler_J2.inverse_transform(noisy_results[:, 0:1])\n",
    "        JZ_pred_from_noise = scaler_JZ.inverse_transform(noisy_results[:, 1:2])\n",
    "        JD_pred_from_noise = scaler_JD.inverse_transform(noisy_results[:, 2:3])\n",
    "        J3_pred_from_noise = scaler_J3.inverse_transform(noisy_results[:, 3:4])\n",
    "\n",
    "        # Compute and store fidelities\n",
    "        compute_and_store_fidelity(fidelity_results['J2'], J2_pred_from_noise, J2_test)\n",
    "        compute_and_store_fidelity(fidelity_results['JZ'], JZ_pred_from_noise, JZ_test)\n",
    "        compute_and_store_fidelity(fidelity_results['JDM'], JD_pred_from_noise, JD_test)\n",
    "        compute_and_store_fidelity(fidelity_results['J3'], J3_pred_from_noise, J3_test)\n",
    "\n",
    "        if g == 1:\n",
    "            '''Plotting scatter plots for noisy data with chi=1.0'''\n",
    "\n",
    "            baselines=[('J2', J2_test, J2_pred_from_noise), \n",
    "            ('JZ', JZ_test, JZ_pred_from_noise), \n",
    "            ('JDMI', JD_test, JD_pred_from_noise), \n",
    "            ('J3', J3_test, J3_pred_from_noise)]\n",
    "\n",
    "            print(test_params[4,:])\n",
    "            plot_baselines(baselines=baselines, name=f'TEST', chi=g, NN_name=name)\n",
    "            predicted_data_NOISYsave = np.concatenate((J2_pred_from_noise, J2_test, JZ_pred_from_noise, JZ_test, JD_pred_from_noise, JD_test, J3_pred_from_noise, J3_test), axis=1)\n",
    "            #np.savetxt('predicted_results_'+name+'_noisy.txt', np.array(predicted_data_NOISYsave))\n",
    "        \n",
    "    return fidelity_results['J2'], fidelity_results['JZ'], fidelity_results['JDM'], fidelity_results['J3'], \\\n",
    "           error_results['J2'], error_results['JZ'], error_results['JDM'], error_results['J3']\n",
    "\n",
    "chi_range  = np.linspace(0,3,31)\n",
    "\n",
    "J2_fids, JZ_fids, JDM_fids, J2Z_fids, J2_errors, JZ_errors, JDM_errors, J2Z_errors= compute_fid_with_seed(dcs_test, y_test_scaled, model, chi_range, pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_fid_stochastic(test, test_params, model_fid, chi_range, pca_i,\n",
    "    MAE, MSE, compute_fidelity, gaussian_noise,\n",
    "    scalers, param_keys, indices,\n",
    "    n_runs=10, run_seed=None, results_csv=\"all_metrics.csv\"\n",
    "    ):\n",
    "    '''Function to compute fidelity metrics over multiple runs and noise levels.\n",
    "    Args:\n",
    "        test (np.array): Test dataset.\n",
    "        test_params (np.array): True parameters for the test dataset.\n",
    "        model_fid (tf.keras.Model): Trained model for predictions.\n",
    "        chi_range (list): List of noise levels to apply.\n",
    "        pca_i (PCA): PCA object for data transformation.\n",
    "        MAE (function): Function to compute Mean Absolute Error.\n",
    "        MSE (function): Function to compute Mean Squared Error.\n",
    "        compute_fidelity (function): Function to compute fidelity.\n",
    "        gaussian_noise (function): Function to add Gaussian noise to data.\n",
    "        scalers (dict): Dictionary of scalers for inverse transformation.\n",
    "        param_keys (list): List of parameter keys.\n",
    "        indices (dict): Dictionary mapping parameter keys to their indices in the predictions.\n",
    "        n_runs (int): Number of runs to perform.\n",
    "        run_seed (int): Seed for random number generation.\n",
    "        results_csv (str): Filename to save the results CSV.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all computed metrics.\n",
    "        \n",
    "    The function performs the following steps:\n",
    "    1. Initializes an empty list to store all metrics.\n",
    "    2. Inverse-transforms the true parameters to their original range.\n",
    "    3. For each run, iterates over the specified noise levels.\n",
    "    4. Adds Gaussian noise to the test data.\n",
    "    5. Transforms the noisy data using PCA and makes predictions using the trained model.\n",
    "    6. Inverse-transforms the predictions to the original parameter range.\n",
    "    7. Computes MAE, MSE, and fidelity for each parameter.'''\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    # computing test values (inverse-transformed to original range)\n",
    "    true_params = {\n",
    "        key: scalers[key].inverse_transform(test_params[:, [idx]])\n",
    "        for key, idx in indices.items()\n",
    "    }\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        print(f\"Starting run {run + 1}/{n_runs}...\")\n",
    "        for g in chi_range:\n",
    "            test_noisy = gaussian_noise(np.copy(test), mu=0.0, std=g * np.std(test), seed=run_seed)\n",
    "\n",
    "            transformed = pca_i.transform(test_noisy)\n",
    "            predictions = model_fid.predict(transformed)\n",
    "\n",
    "            # inverse transforming predictions\n",
    "            pred_params = {\n",
    "                key: scalers[key].inverse_transform(predictions[:, [indices[key]]])\n",
    "                for key in param_keys\n",
    "            }\n",
    "\n",
    "            # computing metrics\n",
    "            mae_vals = {\n",
    "                key: MAE(true=test_params[:, [indices[key]]], pred=predictions[:, [indices[key]]])\n",
    "                for key in param_keys\n",
    "            }\n",
    "\n",
    "            mse_vals = {\n",
    "                key: MSE(true=test_params[:, [indices[key]]], pred=predictions[:, [indices[key]]])\n",
    "                for key in param_keys\n",
    "            }\n",
    "\n",
    "            fid_vals = {\n",
    "                key: compute_fidelity(true=true_params[key], pred=pred_params[key])\n",
    "                for key in param_keys\n",
    "            }\n",
    "\n",
    "            # savong all to metrics list\n",
    "            all_metrics.append({\n",
    "                \"run\": run,\n",
    "                \"g\": g,\n",
    "                **{f\"MAE_{key}\": mae_vals[key] for key in param_keys},\n",
    "                **{f\"MSE_{key}\": mse_vals[key] for key in param_keys},\n",
    "                **{f\"FID_{key}\": fid_vals[key] for key in param_keys},\n",
    "            })\n",
    "\n",
    "    #creating a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(all_metrics)\n",
    "    df.to_csv(results_csv, index=False)\n",
    "    print(f\"Saved all metrics to {results_csv}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "param_keys = ['J2', 'JZ', 'JDM', 'J3']\n",
    "indices = {'J2': 0, 'JZ': 1, 'JDM': 2, 'J3': 3}\n",
    "scalers = {\n",
    "    'J2': scaler_J2,\n",
    "    'JZ': scaler_JZ,\n",
    "    'JDM': scaler_JD,\n",
    "    'J3': scaler_J3\n",
    "}\n",
    "\n",
    "model = tf.keras.models.load_model(f'/Users/35840/Documents/CQM_2024/masters_project/{name}.h5')\n",
    "\n",
    "df_all = compute_fid_stochastic(\n",
    "    test=dcs_test,\n",
    "    test_params=y_test_scaled,\n",
    "    model_fid=model,\n",
    "    chi_range=np.linspace(0, 3, 31),\n",
    "    pca_i=pca,\n",
    "    MAE=MAE,\n",
    "    MSE=MSE,\n",
    "    compute_fidelity=compute_fidelity,\n",
    "    gaussian_noise=gaussian_noise,\n",
    "    scalers=scalers,\n",
    "    param_keys=param_keys,\n",
    "    indices=indices,\n",
    "    n_runs=10,\n",
    "    run_seed=None,\n",
    "    results_csv=f\"{name}.csv\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
